{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the different packages into python\n",
    "\n",
    "import numpy as np               # helps with working with large, multi-dimensional arrays and matrices\n",
    "import pandas as pd              # helps with data manipulation and analysis\n",
    "import matplotlib.pyplot as plt  # helps with plotting graphs, histogram, bar plot, etc\n",
    "import seaborn as sns            # helps with statistical data visualization\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Bank Marketing dataset into python to begin analysis\n",
    "\n",
    "bank=pd.read_csv('C:/Users/Surin/Desktop/Project/bank-additional/bank-additional-full.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the dataset. It contains 41,188 entries and 21 variables.\n",
    "\n",
    "bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics such as count, mean, std, etc done on the dataset with values rounded to the nearest whole number\n",
    "\n",
    "round(bank.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We find the data types for each column. The dataset contains two data types, numeric and categorical. \n",
    "\n",
    "bank.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's check the missing values (if present) in this data. The dataset has zero missing values. \n",
    "\n",
    "nans = bank.shape[0] - bank.dropna().shape[0]\n",
    "print (\"%d rows have missing values in the data\" %nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete any duplicate rows found in the dataset. This left 41,164 entries which means 24 duplicated entries were found and deleted.\n",
    "\n",
    "bank.drop_duplicates(keep = False, inplace = True)\n",
    "bank.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics such as count, mean, std, etc done after the duplicated entries were removed from the dataset with values rounded to the nearest whole number\n",
    "\n",
    "round(bank.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's count the number of unique values from character variables. \n",
    "# It is interesting to note that the month variable only has ten months so no data was provided for two months. \n",
    "\n",
    "cat = bank.select_dtypes(include=['O'])\n",
    "cat.apply(pd.Series.nunique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we'll check the target variable to investigate if this data is imbalanced or not. \n",
    "# We see that almost 89% of the dataset belongs to the 'no' class. This means if we were to take a rough prediction of target variable as 'no', we'll get 89% accuracy. \n",
    "\n",
    "bank.y.value_counts()/bank.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a cross tab of the target variable with job. \n",
    "# With this, we'll try to understand the influence of job on the target variable.\n",
    "\n",
    "round(pd.crosstab(bank['y'],bank['job']).apply(lambda r: r/r.sum(), axis=1)*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a cross tab of the target variable with marital. \n",
    "# With this, we'll try to understand the influence of marital on the target variable.\n",
    "\n",
    "round(pd.crosstab(bank['y'],bank['marital']).apply(lambda r: r/r.sum(), axis=1)*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a cross tab of the target variable with education. \n",
    "# With this, we'll try to understand the influence of education on the target variable.\n",
    "\n",
    "round(pd.crosstab(bank['y'],bank['education']).apply(lambda r: r/r.sum(), axis=1)*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a cross tab of the target variable with default. \n",
    "# With this, we'll try to understand the influence of default on the target variable.\n",
    "\n",
    "round(pd.crosstab(bank['y'],bank['default']).apply(lambda r: r/r.sum(), axis=1)*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a cross tab of the target variable with housing. \n",
    "# With this, we'll try to understand the influence of housing on the target variable.\n",
    "\n",
    "round(pd.crosstab(bank['y'],bank['housing']).apply(lambda r: r/r.sum(), axis=1)*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since about half of the people who subscribed to a term deposit have a housing loan and therefore the other half didn't have a housing loan,housing doesn't give much information on predicting who will subscribe for a term deposit so we delete the housing column from the dataset. \n",
    "\n",
    "del bank['housing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a cross tab of the target variable with loan. \n",
    "# With this, we'll try to understand the influence of loan on the target variable.\n",
    "\n",
    "round(pd.crosstab(bank['y'],bank['loan']).apply(lambda r: r/r.sum(), axis=1)*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a cross tab of the target variable with contact. \n",
    "# With this, we'll try to understand the influence of contact on the target variable.\n",
    "\n",
    "round(pd.crosstab(bank['y'],bank['contact']).apply(lambda r: r/r.sum(), axis=1)*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a cross tab of the target variable with month. \n",
    "# With this, we'll try to understand the influence of month on the target variable.\n",
    "\n",
    "round(pd.crosstab(bank['y'],bank['month']).apply(lambda r: r/r.sum(), axis=1)*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We delete the month column since the dataset doesn't have which year each month is associated with.\n",
    "# It should be noted that the dataset doesn't have any information on the months January and February. \n",
    "\n",
    "del bank['month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a cross tab of the target variable with day_of_week. \n",
    "# With this, we'll try to understand the influence of day_of_week on the target variable.\n",
    "\n",
    "round(pd.crosstab(bank['y'],bank['day_of_week']).apply(lambda r: r/r.sum(), axis=1)*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There was no contact made on Saturdays and Sundays and people were last contacted roughly 20% each weekday. \n",
    "# Therefore the day of the weekday they were last contacted is not important and we can remove this variable.\n",
    "\n",
    "del bank['day_of_week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a new dataset 'bankno', which contains only rows where people did not subscribe for a term deposit (only show target variable 'y' where the outcome is 'no')\n",
    "# The mean duration of a call is 221 seconds.\n",
    "# The mean number of contacts performed before this campaign and with a customer is 0.132.\n",
    "\n",
    "bankno=bank[bank[\"y\"]=='no']\n",
    "bankno.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a new dataset 'bankyes', which contains only rows where people subscribed for a term deposit (only show target variable 'y' where the outcome is'yes')\n",
    "# The mean duration of a call is 553 seconds.\n",
    "# The mean number of contacts performed before this campaign and with a customer is 0.493.\n",
    "# This shows us that the longer the call and the more number of contacts performed to a customer the likelier they will subscribe.\n",
    "\n",
    "bankyes=bank[bank[\"y\"]=='yes']\n",
    "bankyes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplot of the target variable when outcome is 'yes' with campaign (the number of contacts performed during this campaign and for this client)\n",
    "\n",
    "sns.countplot(x=\"campaign\", data=bankyes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a cross tab of the target variable with pdays. \n",
    "# With this, we'll try to understand the influence of pdays on the target variable.\n",
    "\n",
    "round(pd.crosstab(bank['y'],bank['pdays']).apply(lambda r: r/r.sum(), axis=1)*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplot of the target variable when outcome is 'yes' with pdays (the number of days that passed by after the client was last contacted from a previous campaign)\n",
    "\n",
    "sns.countplot(x=\"pdays\", data=bankyes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 999 above means client was not previously contacted. \n",
    "# This value is high for both people who did subscribed and did not subscribe so we can omit this variable from the analysis.\n",
    "\n",
    "del bank['pdays']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a cross tab of the target variable with poutcome. \n",
    "# With this, we'll try to understand the influence of poutcome on the target variable.\n",
    "\n",
    "round(pd.crosstab(bank['y'],bank['poutcome']).apply(lambda r: r/r.sum(), axis=1)*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target variable 'y' has two outcomes 'yes' or 'no'. Here we have the the counts for each outcome.\n",
    "# 36,526 did not subscribed in this dataset whereas 4,638 did subscribe.\n",
    "\n",
    "bank.y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This countplot shows the count of each outcome, 'no' and 'yes' in the target variable.\n",
    "\n",
    "sns.countplot(bank['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit learn accepts data in numeric format. Now, we'll have to convert the character variable into numeric. We'll use the labelencoder function.\n",
    "# In label encoding, each unique value of a variable gets assigned a number, i.e., let's say a variable color has four values ['red','green','blue','pink']. Label encoding this variable will return output as: red = 2 green = 0 blue = 1 pink = 3\n",
    "# Load sklearn and encode all object type variables\n",
    "\n",
    "from sklearn import preprocessing\n",
    "for x in bank.columns:\n",
    "    if bank[x].dtype == 'object':\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(bank[x].values))\n",
    "        bank[x] = lbl.transform(list(bank[x].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the changes applied to the dataset. It can be seen that all character variable were converted to numeric.\n",
    "\n",
    "bank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics such as count, mean, std, etc done on the numeric dataset with values rounded to the nearest whole number\n",
    "\n",
    "round(bank.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target variable 'y' has two outcomes '0' or '1'. '0' represents 'no' as in a subscription wasn't sold to a customer and '1' represents 'yes', a subscription was sold.\n",
    "# Here we have the the counts for each outcome.\n",
    "\n",
    "bank.y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This countplot shows the count of each outcome, '0' and '1' in the target variable.\n",
    "\n",
    "sns.countplot(bank['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the bank dataset into a DataFrame.\n",
    "\n",
    "df=pd.DataFrame(bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a correlation analysis on the dataset \n",
    "\n",
    "C=df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting significant figure to 1 and applying colour backgrounds to the correlation between variables depending on strong positive, strong negative, weak positive, weak negative and no correlation.\n",
    "# Positive correlation between the target variable 'y' and duration and the target variable 'y' and previous.\n",
    "\n",
    "C.style.background_gradient(cmap='coolwarm').set_precision(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our dataset into its attributes and labels\n",
    "\n",
    "X = bank.drop('y', axis = 1)\n",
    "y = bank['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what y looks like\n",
    "\n",
    "y.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will divide our dataset into training and test splits. The training data will be used to train the logistic regression model and the test data will be used to evaluate the performance of the model.\n",
    "# Splits 90% of the dataset into our training set and the other 10% into test data.\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying standard scaling to get optimized result\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the logistic regression model and start making predictions using Scikit-Learn\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train,y_train)\n",
    "predictions = logmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When our logistic regression model predicted y is going to be ‘no’ (0), it is accurate 93% of the time, ‘yes’ (1) is predicted with 69% precision. \n",
    "# In Recall, if the client didn’t subscribe to a term deposit - ‘no’ in the test set our logistic regression model can identify it 98% of the time; if the client did subscribe - ‘yes’ is predicted 40% of the time.\n",
    "# Logistic regression model gets 90% accuracy overall.\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "# Splits 90% of the dataset into our training set and the other 10% into test data.\n",
    "# The random_state parameter which controls the randomness of the bootstrapping of the samples used when building trees and the sampling of the features to consider when looking for the best split at each node was set to 0. \n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X, y, test_size = 0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying standard scaling to get optimized result\n",
    "\n",
    "sc2 = StandardScaler()\n",
    "X2_train = sc2.fit_transform(X2_train)\n",
    "X2_test = sc2.fit_transform(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the random forest classifier and start making predictions using Scikit-Learn\n",
    "# The number of trees (n_estimators) was set to 200 since if there are more trees it will not allow overfitting trees in the model.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=200)\n",
    "rfc.fit(X2_train, y2_train)\n",
    "pred_rfc = rfc.predict(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When our random forest classifier predicted y is going to be ‘no’ (0), it is accurate 94% of the time, ‘yes’ (1) is predicted with 63% precision. \n",
    "# In Recall, if the client didn’t subscribe to a term deposit - ‘no’ in the test set our random forest classifier can identify it 96% of the time; if the client did subscribe - ‘yes’ is predicted 52% of the time.\n",
    "# Random Forest Classifier gets 91% accuracy overall.\n",
    "\n",
    "print(classification_report(y2_test, pred_rfc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
